---
title: "Data Analysis about graduate admission"
author: "Yansong Li"
date: "4/10/2023"
output: pdf_document
---

Note: This file only contains the procedures for performing analysis in R. Please refer to the link posted on the resume to access the complete data analysis report.


```{r}
library(tidyverse)
library(ggplot2)
library(latex2exp)
library(kableExtra)
library(MASS)
library(car)
install.packages("gridExtra")
library(gridExtra)
install.packages("glmnet")
library(glmnet)
library(gglasso)
library(psych)
library(pls)
library(rms)
library(MPV)
library(Matrix)
```


### Load dataset and show the summary of whole dataset


```{r, echo=FALSE}
data=read.csv("Admission_Predict.csv")
summary(data)
```


### Data cleaning: Removing all non-available data from datasetg 


```{r}
data <- data %>%
  filter(!is.na(data$GRE.Score))%>%
  filter(!is.na(data$TOEFL.Score)) %>%
  filter(!is.na(data$University.Rating)) %>%
  filter(!is.na(data$SOP)) %>%
  filter(!is.na(data$LOR)) %>%
  filter(!is.na(data$CGPA)) %>%
  filter(!is.na(data$Research)) %>%
  filter(!is.na(data$Chance.of.Admit))
```


### Split dataset into two parts which are train data and test data 


```{r}
set.seed(1007208406)
rows <- sample(1:400, 300, replace=FALSE) 
train<- data[rows,]
test = data[-rows,]
```


### Summary data of train dataset


```{r}
summary(train)
```


## Show the distribution of response variable in train dataset


```{r, echo=FALSE}
hist(train$Chance.of.Admit,col="light blue", xlab='Chance of Admit', main="Distribution of Response Variable")
```


## Check linearity with predictor 1 (GRE Score) in train dataset


```{r, echo=FALSE}
ggplot(train, aes(GRE.Score, Chance.of.Admit)) +
  geom_point() +
  geom_smooth(se = FALSE,method="lm") +
  labs(
    title = "Linearity With Predictor 1 (GRE Score)",
    subtitle = "The change of admit increases as GRE score goes up",
    x="GRE Score",
    y="Chance of Admit",
    caption = "Data from Kaggle"
  ) +
  theme_bw()
```


## Check linearity with predictor 2 (TOEFL Score) in train dataset


```{r, echo=FALSE}
ggplot(train, aes(TOEFL.Score, Chance.of.Admit)) +
  geom_point() +
  geom_smooth(se = FALSE,method="lm") +
  labs(
   title = "Linearity With Predictor 2 (TOEFL Score)",
   subtitle = "The change of admit increases as TOEFL score goes up",
    x="TOEFL Score",
    y="Chance of Admit",
   caption = "Data from Kaggle") +
  theme_bw()
```


## Check linearity with predictor 3 (Undergraduate GPA) in train dataset


```{r, echo=FALSE}
ggplot(train, aes(CGPA, Chance.of.Admit)) +
  geom_point() +
  geom_smooth(se = FALSE,method="lm") +
  labs(
    title = "Linearity With Predictor 3 (Undergraduate GPA)",
    subtitle = "The change of admit increases as TOEFL score goes up",
    x="Undergraduate CGPA",
    y="Chance of Admit",
    caption = "Data from Kaggle") +
  theme_bw()
```


## Check linearity with predictor 4 (University Rating) in train dataset


```{r, echo=FALSE}
library(ggplot2)
p <- ggplot(train, aes(x=as.factor(University.Rating), y=Chance.of.Admit, fill = as.factor(University.Rating)))+
  geom_boxplot()+
  labs(title="Linearity With Predictor 4 (University Rating)",
     x="University Rating",
     y="Chance of Admit")
p + labs(fill = "University Rating")
```


## Check linearity with predictor 5 (Research Experience) in train dataset


```{r, echo=FALSE}
p <- ggplot(train, aes(x=as.factor(Research), y=Chance.of.Admit, fill = as.factor(Research)))+
geom_boxplot()+
labs(title="Linearity With Predictor 5 (Research Experience)",
     subtitle = "0 means NO, 1 means Yes",
     x="Research Experience",
     y="Chance of Admit") 
p + labs(fill = "Research Experience")
```


## Check linearity with predictor 6 (Strength of Statement of Purpose) in train dataset


```{r, echo=FALSE}
p <- ggplot(train, aes(x=as.factor(SOP), y=Chance.of.Admit, fill = as.factor(SOP)))+
geom_boxplot()+
labs(title="Linearity With Predictor 6 (Strength of Statement of Purpose)",
     subtitle = "1 means Weak, 5 means Stronge",
     x="Strength of Statement of Purpose",
     y="Chance of Admit") 
p + labs(fill = "Strength of Statement of Purpose")
```


## Check linearity with predictor 7 (Strength of Letter of Recommendation) in train dataset


```{r, echo=FALSE}
p <- ggplot(train, aes(x=as.factor(LOR), y=Chance.of.Admit, fill = as.factor(LOR)))+
geom_boxplot()+
labs(title="Linearity With Predictor 7 (Strength of Letter of Recommendation)",
     subtitle = "1 means Weak, 5 means Stronge",
     x="Strength of Letter of Recommendation",
     y="Chance of Admit") 
p + labs(fill = "Strength of Letter of Recommendation")
```

For the response variable, the histogram shows the overall distribution is a little left skewed. Thus, we may do a boxcox transformation in further steps. For predictor 1, 2 and 3, they are numerical, so we use scatterplot to present data. All three scatterplots show obvious positively linear relationship. In addition, the population of response variable at any value of the predictor in each graph has almost same spread. For predictor 4, 5, 6 and 7, they are categorical, so we use side-by-side boxplot to present. In these four boxplots, each category in each predictor has different distribution regarding to response variable and the vertical length of each boxplot in each graph are almost same, so the population of response at any category of the predictor has almost same spread. In addition, all data are randomly selected for all predictors.


# Model Selection:

### The First model (with all predictors) 

```{r}
model_full <- lm(Chance.of.Admit ~ GRE.Score + TOEFL.Score + University.Rating + SOP + LOR + CGPA + Research, data = train)
anova(model_full)
summary(model_full)
```


### The Second model (Based on the AIC selection)

```{r}
model.lm <- lm(Chance.of.Admit ~ ., data = train[,c(-1)])
summary(model.lm)
n <- nrow(train)
sel.var.aic <- step(model.lm, trace =20, k = 2, direction = "both")
sel.var.aic <- attr(terms(sel.var.aic), "term.labels")
sel.var.aic
```

```{r}
model_se <- lm(Chance.of.Admit ~ GRE.Score + TOEFL.Score + University.Rating + LOR + CGPA + Research, data = train)
summary(model_se)
```


### The Third model (Based on the BIC selection)

```{r}
model.lm <- lm(Chance.of.Admit ~ ., data = train[,c(-1)])
summary(model.lm)
n <- nrow(train)
sel.var.bic <- step(model.lm, trace =20, k = log(n), direction = "both")
sel.var.bic <- attr(terms(sel.var.bic), "term.labels")
sel.var.bic
```

```{r}
model_th <- lm(Chance.of.Admit ~ GRE.Score + LOR + CGPA, data = train)
summary(model_th)
```


# The Fourth model (Based on the LAASSO selection)

```{r}
set.seed(1007208406)
cv.out <- cv.glmnet(x = as.matrix(train[,2:8]), y = train$Chance.of.Admit, standardize = T, alpha = 1)
plot(cv.out)
best.lambda <- cv.out$lambda.1se
best.lambda
co <- coef(cv.out, s = "lambda.1se")
thresh <- 0.00
inds <- which(abs(co) > thresh)
variables <- row.names(co)[inds]
sel.var.lasso <- variables[!(variables %in% '(Intercept)')]
sel.var.lasso
```

```{r}
model_fo <- lm(Chance.of.Admit ~ GRE.Score + TOEFL.Score + University.Rating + LOR + CGPA + Research, data = train)
summary(model_se)
```


### Summary table for four models

```{r}
select = function(model, n)
{
  SSres <- sum(model$residuals^2)
  Rsq <- summary(model)$r.squared
  Rsq_adj <- summary(model)$adj.r.squared
  p <- length(model$coefficients) - 1
  AIC <- n*log(SSres/n) + 2*p    
  AICc <- AIC + (2*(p+2)*(p+3)/(n-p-1))
  BIC <- n*log(SSres/n) + (p+2)*log(n)    
  res <- c(SSres, Rsq, Rsq_adj, AIC, AICc, BIC)
  names(res) <- c("SSres", "Rsq", "Rsq_adj", "AIC", "AIC_c", "BIC")
  return(res)
}


s1 <- select(model_full, nrow(train))
s1
s2 <- select(model_se, nrow(train))
s2
s3 <- select(model_th, nrow(train))
s3
s4 <- select(model_fo, nrow(train))
s4
```


### We get a relatively better model

```{r}
final_mod <- lm(Chance.of.Admit ~ GRE.Score + TOEFL.Score + University.Rating + LOR + CGPA + Research, data = train[,-c(1)])
summary(final_mod)
```

Compare indicative values among four models, it is obvious that model 2 and model 4 have the highest $R^2$ and $R^2_{adj}$ and the smallest AIC.


# Patial F test (Result shows to remove insignificant predictors -- University Rating)

```{r}
final_mod <- lm(Chance.of.Admit ~ GRE.Score + TOEFL.Score + University.Rating + LOR + CGPA + Research, data = train[,-c(1)])
final_mod1 <- lm(Chance.of.Admit ~ GRE.Score + TOEFL.Score + LOR + CGPA + Research, data = train[,-c(1)])

anova(final_mod, final_mod1)
```

Since p-value is 0.1367 which is greater than 0.05, so it indicates that removing predictor" University Rating" can make our model better. Therefore, we should use it as our final model.


# model check
### Anova and summary table for this better model

```{r}
anova(final_mod1)
summary(final_mod1)
```

The p-value for all predictors are significant


### Check assumptions of this model.

```{r}
par(mfrow=c(2,2))
plot(final_mod1,1)
plot(final_mod1,2)
plot(final_mod1,3)
plot(final_mod1,4)
```

Since the QQ-plot tells shows some points are not on the QQ line, so it is not perfectly normal distributiion, so we need to use box-cox transformation to do it.


### box-cox transformation

```{r}
summary(powerTransform(cbind(train$Chance.of.Admit)))
```


###  transform response variable

```{r}
train$Chance.of.Admit <- train$Chance.of.Admit^(2)
test$Chance.of.Admit <- test$Chance.of.Admit^(2)
```


### Create transformed model (final model)

```{r}
final_mod2 <- lm(Chance.of.Admit ~ GRE.Score + TOEFL.Score + LOR + CGPA + Research, data = train[,-c(1)])
anova(final_mod2)
summary(final_mod2)
```


### Check Assumptions again using final model

```{r}
par(mfrow=c(2,2))
plot(final_mod2,1)
plot(final_mod2,2)
plot(final_mod2,3)
plot(final_mod2,4)
```


### Check outliers

```{r}
r <- rstandard(final_mod2)
length(which(rstandard(final_mod2) < -4 | rstandard(final_mod2) > 4))
```


### Check leverage point

```{r}
p1 <- length(coef(final_mod2))-1
n1 <- length(train$Chance.of.Admit)

h <- hatvalues(final_mod2)
length(which(hatvalues(final_mod2) > 2*(p1+1)/n1))
```


### Check influencial point

```{r}
D <- cooks.distance(final_mod2)
Dcutoff <- qf(0.5, p1+1, n1-p1-1)
length(which(D > Dcutoff))
```


### Check VIF

```{r}
vif(final_mod2)
```

The VIF of each predictor is smaller than 5 which implies that there is no multicollinearity existed in final model.


### Check validation of the model (Predicted error)

```{r}
pred.y <- predict(final_mod2, newdata = test, type = "response")

mean((test$Chance.of.Admit - pred.y)^2)
```

the error is 0.0078 which is almost equal to 0, so it is a valid model.


### For final model, switch data from train dataset to test dataset,and then check assumptions again

```{r}
final_mod2 <- lm(Chance.of.Admit ~ GRE.Score + TOEFL.Score + LOR + CGPA + Research, data = test[,-c(1)])

# Check Assumptions again using transformed model

par(mfrow=c(2,2))
plot(final_mod2,1)
plot(final_mod2,2)
plot(final_mod2,3)
plot(final_mod2,4)

# Check outliers
r <- rstandard(final_mod2)
length(which(rstandard(final_mod2) < -2 | rstandard(final_mod2) > 2))


# Check leverage point
p1 <- length(coef(final_mod2))-1
n1 <- length(test$Chance.of.Admit)

h <- hatvalues(final_mod2)
length(which(hatvalues(final_mod2) > 2*(p1+1)/n1))


# Check influencial point
D <- cooks.distance(final_mod2)
Dcutoff <- qf(0.5, p1+1, n1-p1-1)
length(which(D > Dcutoff))
```

### Check VIF
```{r}
vif(final_mod2)
```

For most predictors, VIF is smaller than 5, there is only one predictor that is CGPA with VIF = 6.2. The reason of causing this would be that the dataset of test dataset is small and the observation are randomly selected. However, in overall, the characteristics of test dataset is  similar to train dataset. therefore, the model is for general, not just specialized for train dataset


### Show final model regression coefficient 

```{r}
final_mod2 <- lm(Chance.of.Admit ~ GRE.Score + TOEFL.Score + LOR + CGPA + Research, data = train[,-c(1)])
summary(final_mod2)
```

Finally, We build the final Model  

$$\hat{Chance.of.Admission}^{2} = -2.46 + 0.003GRE.Score+ 0.005TOEFL.Score+0.03LOR+ 0.17CGPA+0.03I(Research)$$








